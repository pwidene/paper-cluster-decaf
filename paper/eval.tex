\section{Evaluation}
\label{s:eval}

The goals of the evaluation are to demonstrate the \ada{state goals
  clearly to set correct expectations, see if this is correct or needs
  to be expanded:} the feasibility of reusing
  SuperGlue components across workloads, and the ability of the
  SuperGlue components to maintain similar performance and scaling
levels as ``hand-tuned'' workflow compositions. 

The evaluation is performed on Titan, the Cray XK7 machine at Oak Ridge
National Laboratory. It consists of 18,688 nodes each with 1 16-core AMD
Opteron CPU and 32 GB of RAM. The interconnect is a Gemini network. There is an
attached Nvidia Kepler K20X GPU with an additional 6 GB of memory on every
node.

%The evaluation is performed on Rhea, a capacity cluster at Oak Ridge National
%Laboratory. It consists of 512 nodes each with two 8-core 2.0 GHz Intel Xeon
%processors with Hyper-Threading and 128 GB of RAM. The interconnect is FDR
%Infiniband. For storage, Rhea uses OLCF's 32 PB Atlas Lustre parallel file
%system.

\subsection{Strong Scaling Experiments}


\begin{figure}
  \centering
  \vspace{-0.25in}
  \input{data/lmp-sel-strong}
  \vspace{-0.15in}
  \input{data/lmp-mag-strong}
  \vspace{-0.17in}
  \input{data/lmp-hist-strong}
  %  }
  %
  \vspace{-0.05in}
  \caption{SuperGlue strong scaling in the LAMMPS workflow.
    Completion time (secs) of a full timestep and of the data transfer
    portion of the same timestep are plotted against process size.}
  \label{fig:lammps-strong}
  \vspace{-0.18in}
\end{figure}

\begin{figure}
  \centering
  \vspace{-0.17in}
  \input{data/gtcp-sel1-strong}
  \vspace{-0.17in}
  \input{data/gtcp-dimr-strong}
  \vspace{-0.06in}
  \caption{SuperGlue strong scaling in the GTCP workflow. Whole timestep
    completion time (secs) for Select and both instances of Dim-Reduce used in
    the workflow are plotted against process size}
  \label{fig:gtcp-strong}
  \vspace{-0.25in}
\end{figure}

\ada{add axis labels and units to plots. }
  

\begin{table*}[tbp]
%\vspace{-0.15in}
\centering
\caption{LAMMPS Evaluation Configuration Settings}
\label{tab:eval-strong-lammps}
\vspace{-0.15in}
\begin{tabular}{|l|l|l|l|l|}
\hline
Component Test & LAMMPS Procs & Select Procs & Magnitude Procs & Histogram Procs \\
\hline
Select & 256 & $x$ & 16 & 8\\
\hline
Magnitude & 256 & 60 & $x$ & 8\\
\hline
Histogram & 256 & 32 & 16 & $x$\\
\hline
\end{tabular}
%\vspace{-0.15in}
\end{table*}

%LAMMPS setups:
%Select is 256:x:16:8
%Magnitude is 256:60:x:8
%Histogram is 256:32:16:x

\begin{table*}[tbp]
\centering
\caption{GTCP Evaluation Configuration Settings}
\label{tab:eval-strong-gtcp}
\vspace{-0.15in}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Component Test & GTCP Procs & Select Procs & Dim-Reduce 1 & Dim-Reduce 2 & Histogram Procs \\
\hline
Select & 64 & $x$ & 4 & 4 & 4\\
\hline
Dim-Reduce 1 & 128 & 32 & $x$ & 16 & 16\\
\hline
Dim-Reduce 2 & 128 & 32 & 16 & $x$ & 16\\
\hline
%Histogram & 128 & 34 & 24 & 24 & $x$\\
%\hline
\end{tabular}
\vspace{-0.07in}
\end{table*}

%GTCP setups:
%Select is 64:x:4:4:4
%Dim-Reduce1 is 128:32:x:16:16
%Dim-Reduce2 is 128:32:16:x:16
%Histogram is 128:34:24:24:x

To understand the strong scaling behavior exhibited by the components in
different scenarios, we carried out strong scaling measurements of the
components in both the LAMMPS and GTCP workflows.
To do this, we varied the process size of a single component at
a time while fixing that of the other components involved in the workflow,
and using a fixed output size from the driving simulation.  We
determined reasonable process sizes for the fixed-size components using
preliminary testing. 

The results are illustrated in~\autoref{fig:lammps-strong} and~\autoref{fig:gtcp-strong}.
Each point shows
the completion time for a single time step arbitrarily chosen in the middle of
the execution of the workflow. Depicted below the strong scaling curves
in~\autoref{fig:lammps-strong}
are the data transfer times. That is, these points
show the portion of the timestep
completion time spent by the components waiting to receive requested data.
The workflow configurations (process counts) used to obtain these measurements are shown
in~\autoref{tab:eval-strong-lammps} and~\autoref{tab:eval-strong-gtcp}.
The global output of the simulation in the LAMMPS workflow
is \SI{1.28}{\giga\byte}. In the GTCP workflow, the Select
measurements are taken using a \SI{900}{\mega\byte} output
from the simulation, and those of Dim-Reduce using a
\SI{3.8}{\giga\byte} output.
The scale used for these results is small compared to the
scale at which these simulations can be run due to
the limited time and resources available 
for the numerous complete workflows executions required
to obtain meaningful strong scaling results.

These results provide valuable information about the components.
First, they show that the components exhibit regular strong
scaling behavior. That is, 
a linear domain of scalability is followed by a turning point and an
eventual flattening out of the performance, where the benefit
of adding more processes dwindles.
That there exists a linear domain means that given
a particular workload, users can select SuperGlue process sizes
that match their resources and performance requirements.
The flat domain is long and does not show any drastic reversal
of performance. Without knowing the full strong
scaling characteristics of the SuperGlue components used in a particular workflow,
a user can safely guess a process size to use for a particular component,
using the strong scaling results from the same component with a similar
workload size, without incurring unreasonable overhead.

The turning point of scalability of a component
is not necessarily determined by
its per-process workload size.
In~\autoref{fig:lammps-strong}, the turning point of Select
occurs at a per-process workload of around \SI{32}{\mega\byte}.
Here, the global data size sent to Select
is \SI{1.28}{\giga\byte}.
In a separate set of measurements using the GTCP workflow,
the turning point of Select occurs at a per-process workload
of \SI{113}{\mega\byte}, where the global dataset size
was \SI{3.8}{\giga\byte}. Therefore, global workload size
is a factor in determining the turning point of scalability
of the SuperGlue components.

While there is communication overhead in the computation itself,~\autoref{fig:lammps-strong} shows the majority of the communication
overhead, i.e., the time spent on data transfer between components,
is small compared to the total per-timestep execution time of
the SuperGlue components. This supports the idea
of assembling workflows using numerous, simple,
generic components.

\subsection{Weak Scaling Experiments}

Additional experiments are performed for the GTCP workflow to determine the
weak scaling performance for the components. The configurations for these experiments
are presented in~\autoref{tab:eval-weak-gtcp-1}. The performance results are
presented in~\autoref{tab:eval-weak-gtcp-2}. To read the tables, match the
rows. For example, the first row in~\autoref{tab:eval-weak-gtcp-1} corresponds
to the performance results in row 1 in~\autoref{tab:eval-weak-gtcp-2}.
These runs use SuperGlue process sizes for which the per-process
workloads reside near the turning points in the
strong scaling results presented above.

Overall, the components and the overall workflow
exhibit very promising weak scaling behavior.
While there are slightly different per-process
data sizes in each row of the table for both
per-simulation process and per-SuperGlue process,
there is little variation in the timestep completion
times of the SuperGlue components and in the end-to-end
completion times of the entire workflow for different
workload sizes.

{\em Select} gets the full brunt of the total data size. Dividing the process
count into the total data size shows that on average, it is roughly the same
for weak scaling. The other components exhibit similar performance consistency.
Also note that these are not exactly identical ratios or counts.  The data size
per GTCP process is maintained as it is scaled. The amount of data each
SuperGlue component process varies a bit. Also note that there is not a fixed
n-1 ratio required for any of the components. Instead, an m-n mapping works
correctly.

%Process count ratios
%GTCP	GTCP		GTCP
%vs	vs		vs
%Select	Dim-Reduce	Histogram
%6.4	10.67		32
%5.25	8.4		41
%8.67	11.14		34
%9.36	12.31		46.8

\begin{table*}[tbp]
%\vspace{-0.15in}
\centering
\caption{GTCP Weak Scaling Evaluation Configuration Settings}
\label{tab:eval-weak-gtcp-1}
\vspace{-0.15in}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Configuration & GTCP Procs & Select Procs & Dim-Reduce 1 & Dim-Reduce 2 & Histogram Procs & Total Data Size & End-to-End Time\\
\hline
C1 & 64 & 10 & 6 & 6 & 2 & 918,303,680 & 92.724\\
\hline
C2 & 84 & 16 & 10 & 10 & 2 & 1,434,599,936 & 115.232\\
\hline
C3 & 156 & 18 & 14 & 14 & 4 & 2,065,583,520 & 97.266\\
\hline
C4 & 234 & 25 & 19 & 19 & 5 & 2,811,256,000 & 96.359\\
\hline
\end{tabular}
\vspace{-0.15in}
\end{table*}

\begin{table}[tbp]
%\vspace{-0.10in}
\centering
\caption{GTCP Weak Scaling Component Performance}
\label{tab:eval-weak-gtcp-2}
\vspace{-0.15in}
\begin{tabular}{|p{0.1 in}|p{0.67 in}|p{0.65 in}|p{0.65 in}|p{0.65 in}|}
\hline
 & Select Average Time & Dim-Reduce 1 Average Time & Dim-Reduce 2 Average Time & Histogram Average Time\\
\hline
C1 & 1.55 & 1.58 & 1.34 & 0.6\\
\hline
C2 & 2.34 & 1.58 & 1.72 & 0.78\\
\hline
C3 & 2.31 & 1.73 & 1.54 & 0.713\\
\hline
C4 & 2.19 & 1.76 & 1.68 & 0.89\\
\hline
\end{tabular}
\vspace{-0.25in}
\end{table}

