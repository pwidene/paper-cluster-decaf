@article{bertram:2006:kepler,
 author = {Bertram Lud\"{a}scher and Ilkay Altintas and Chad Berkley and Dan
Higgins and Efrat Jaeger and Matthew Jones and Edward A. Lee and Jing Tao and
Yang Zhao},
 title = {Scientific Workflow Management and the Kepler System:
Research Articles},
 journal = {Concurr. Comput. : Pract. Exper.},
 volume = {18},
 number = {10},
 year = {2006},
 issn = {1532-0626},
 pages = {1039--1065},
 doi = {http://dx.doi.org/10.1002/cpe.v18:10},
 publisher = {John Wiley and Sons Ltd.},
 address = {Chichester, UK, UK}
 }

@ARTICLE{Malewicz:2006:dagman,
title={A Tool for Prioritizing {DAGMan} Jobs and Its Evaluation},
author={Malewicz, G. and Foster, I. and Rosenberg, A.L. and Wilde, M.},
journal={High Performance Distributed Computing, 2006 15th IEEE International Symposium on},
year={2006},
month={0-0 },
volume={},
number={},
pages={156-168},
abstract={It is often difficult to perform efficiently a collection of jobs with complex job dependencies due to temporal unpredictability of the grid. One way to mitigate the unpredictability is to schedule job execution in a manner that constantly maximizes the number of jobs that can be sent to workers. A recently developed scheduling theory provides a basis to meet that optimization goal. Intuitively, when the number of such jobs is always large, high parallelism can be maintained, even if the number of workers changes over time in an unpredictable manner. In this paper we present the design, implementation, and evaluation of a practical scheduling tool inspired by the theory. Given a DAGMan input file with interdependent jobs, the tool prioritizes the jobs. The resulting schedule significantly outperforms currently used schedules under a wide range of system parameters, as shown by simulation studies. For example, a scientific data analysis application, AIRSN, was executed at least 13\% faster with 95\% confidence. An implementation of the tool was integrated with the Condor high-throughput computing system},
keywords={Internet, grid computing, schedulingCondor high-throughput computing system, DAGMan job scheduling, Internet-based computation, job dependency, scheduling tool, scientific data analysis application},
doi={10.1109/HPDC.2006.1652146},
ISSN={1082-8907}, }

@inproceedings{nisar:2008:staging,
 author = {Nisar,, Arifa and Liao,, Wei-keng and Choudhary,, Alok},
 title = {Scaling Parallel {I/O} Performance Through {I/O} Delegate and Caching System},
 booktitle = {SC '08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing},
 year = {2008},
 isbn = {978-1-4244-2835-9},
 pages = {1--12},
 location = {Austin, Texas},
 doi = {http://doi.acm.org.www.library.gatech.edu:2048/10.1145/1413370.1413380},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 }

@inproceedings{abbasi:2009:datastager,
  author    = {Hasan Abbasi and
               Matthew Wolf and
               Greg Eisenhauer and
               Scott Klasky and
               Karsten Schwan and
               Fang Zheng},
  title     = {DataStager: scalable data staging services for petascale
               applications},
  booktitle = {HPDC},
  year      = {2009},
  pages     = {39-48},
  ee        = {http://doi.acm.org/10.1145/1551609.1551618},
  crossref  = {DBLP:conf/hpdc/2009},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/hpdc/2009,
  editor    = {Dieter Kranzlm{\"u}ller and
               Arndt Bode and
               Heinz-Gerd Hegering and
               Henri Casanova and
               Michael Gerndt},
  title     = {Proceedings of the 18th ACM International Symposium on High
               Performance Distributed Computing, HPDC 2009, Garching,
               Germany, June 11-13, 2009},
  booktitle = {HPDC},
  publisher = {ACM},
  year      = {2009},
  isbn      = {978-1-60558-587-1},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{ober:seismic,
        Abstract = {
A key to reducing the risks and costs associated with oil and gas exploration is the fast, accurate imaging of complex geologies, such as salt domes in the Gulf of Mexico and overthrust regions in U.S. onshore regions. Prestack depth migration generally yields the most accurate images, and one approach to this is to solve the scalar-wave equation using finite differences.
Current industry computational capabilities are insufficient for the application of finite-difference, 3-D, prestack, depth-migration algorithms. A 3-D seismic data can be several terabytes in size, and the multiple runs necessary to refine the velocity model may take many years. The oil companies and seismic contractors need to perform complete velocity field refinements in weeks and single iterations overnight. High-performance computers and state-of-the-art algorithms and software are required to meet this need.
As part of an ongoing ACTI project funded by the U.S. Department of Energy, we have developed a finite-difference, 3-D prestack, depth-migration code for the Intel Paragon. The goal of this work is to demonstrate that massively parallel computers (thousands of processors) can be used efficiently for seismic imaging, and that sufficient computing power exists (or soon will exist) to make finite-difference, prestack, depth migration practical for oil and gas exploration.},
        Author = {Curtis C. Ober and Ron A. Oldfield and David E. Womble and John Van Dyke},
        Date-Added = {2009-10-28 15:13:18 -0600},
        Date-Modified = {2010-11-12 16:11:32 -0700},
        Doi = {DOI: 10.1016/S0898-1221(98)00033-9},
        Issn = {0898-1221},
        Journal = {Computers \& Mathematics with Applications},
        Keywords = {Oil search},
        Note = {Advanced Computing on Intel Architectures},
        Number = {7},
        Pages = {65 - 72},
        Title = {Seismic imaging on the {I}ntel {P}aragon},
        Url = {http://www.sciencedirect.com/science/article/B6TYJ-3SYXG2V-7/2/ce76e3a4ad1f2d8d17e1bdc0751ce513},
        Volume = {35},
        Year = {1998},
        Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/B6TYJ-3SYXG2V-7/2/ce76e3a4ad1f2d8d17e1bdc0751ce513},
        Bdsk-Url-2 = {http://dx.doi.org/10.1016/S0898-1221(98)00033-9}}

@inproceedings{lofstead:2009:adaptable,
  crossref="lofstead:adaptible",
}

@inproceedings{lofstead:adaptible,
        Address = {Rome, Italy},
        Author = {Jay Lofstead and Fang Zheng and Scott Klasky and Karsten Schwan},
        Booktitle = {Proceedings of the International Parallel and Distributed Processing Symposium},
        Date-Added = {2009-10-14 13:52:24 -0600},
        Date-Modified = {2009-10-14 13:55:04 -0600},
        Keywords = {application programmer interface, pario-bib},
        Title = {Adaptable, metadata rich {IO} methods for portable high performance {IO}},
        Year = {2009}}

@INPROCEEDINGS{zheng:2010:predata,
    author = {Fang Zheng and Hasan Abbasi and Ciprian Docan and Jay Lofstead and Scott Klasky and Qing Liu and Manish Parashar and Norbert Podhorszki and Karsten Schwan and Matthew Wolf},
    title = {{PreDatA }- Preparatory Data Analytics on {Peta-Scale} Machines},
    abstract = {Peta-scale scientific applications running on High
End Computing (HEC) platforms can generate large volumes
of data. For high performance storage and in order to be
useful to science end users, such data must be organized in
its layout, indexed, sorted, and otherwise manipulated for subsequent
data presentation, visualization, and detailed analysis.
In addition, scientists desire to gain insights into selected data
characteristics `hidden' or `latent' in these massive datasets
while data is being produced by simulations. PreDatA, short for
Preparatory Data Analytics, is an approach to preparing and
characterizing data while it is being produced by the large scale
simulations running on peta-scale machines. By dedicating
additional compute nodes on the machine as `staging' nodes
and by staging simulations' output data through these nodes,
PreDatA can exploit their computational power to perform
select data manipulations with lower latency than attainable
by first moving data into file systems and storage. Such intransit
manipulations are supported by the PreDatA middleware
through asynchronous data movement to reduce write
latency, application-specific operations on streaming data that
are able to discover latent data characteristics, and appropriate
data reorganization and metadata annotation to speed up
subsequent data access. PreDatA enhances the scalability and
exibility of the current I/O stack on HEC platforms and
is useful for data pre-processing, runtime data analysis and
inspection, as well as for data exchange between concurrently
running simulations.},
    booktitle = {In Proceedings of 24th IEEE International Parallel and Distributed Processing Symposium, April, Atlanta, Georgia},
    year = {2010}
}

@inproceedings{mullender:pegasus,
        Author = {Sape J. Mullender and Ian M. Leslie and Derek McAuley},
        Booktitle = {Proceedings of the 1994 Summer USENIX Technical Conference},
        Comment = {The Pegasus project, the Nemesis operating system, and the Nematode prototype. Nemesis is a single-address-space operating system with multiple protection domains. It is specifically designed for multimedia support, and they have some interesting scheduler support for that. They use a variant of a log-structured file system. They use naming like Plan 9.},
        Keywords = {operating system, multimedia, single address space},
        Pages = {209--219},
        Private = {Skimmed. Not in files; on shelf.},
        Title = {Operating-System Support for Distributed Multimedia},
        Year = {1994}}

@article{Moreland:2008:paraview,
  author={K Moreland and D Lepage and D Koller and G Humphreys},
  title={Remote rendering for ultrascale data},
  journal={Journal of Physics: Conference Series},
  volume={125},
  number={1},
  pages={012096},
  url={http://stacks.iop.org/1742-6596/125/i=1/a=012096},
  year={2008},
  abstract={The mission of the SciDAC Institute for Ultrascale Visualization is to address the upcoming petascale visualization challenges. As we move to petascale computation, we are seeing a trend not only in the growth but also in the consolidation of computing resources. As the distances between user and petascale visualization resources grow, the expected performance of the network degrades, especially with respect to latency. In this paper we will explore a technique for remote visualization that leverages unstructured lumigraph rendering. This technique will provide an interactive rendering experience regardless of the network performance to the remote visualization resource. The unstructured lumigraph rendering can also replace many of the other level-of-detail techniques currently used that have problems that are exasperated by petascale data.}
}

@INPROCEEDINGS{Riedel:2007:visit,
author={Riedel, M. and Eickermann, T. and Habbinga, S. and Frings, W. and Gibbon, P. and Mallmann, D. and Wolf, F. and Streit, A. and Lippert, T. and Schiffmann, W. and Ernst, A. and Spurzem, R. and Nagel, W.E.},
booktitle={e-Science and Grid Computing, IEEE International Conference on}, title={Computational Steering and Online Visualization of Scientific Applications on Large-Scale HPC Systems within e-Science Infrastructures},
year={2007},
month={dec.},
volume={},
number={},
pages={483 -490},
abstract={In the past several years, many scientific applications from various domains have taken advantage of e-science infrastructures that share storage or computational resources such as supercomputers, clusters or PC server farms across multiple organizations. Especially within e-science infrastructures driven by high-performance computing (HPC) such as DEISA, online visualization and computational steering (COVS) has become an important technique to save compute time on shared resources by dynamically steering the parameters of a parallel simulation. This paper argues that future supercomputers in the Petaflop/s performance range with up to 1 million CPUs will create an even stronger demand for seamless computational steering technologies. We discuss upcoming challenges for the development of scalable HPC applications and limits of future storage/IO technologies in the context of next generation e- science infrastructures and outline potential solutions.},
keywords={PC server farm;computational resource;computational steering technology;e-science infrastructure;high performance computing;online visualization;parallel simulation;scientific application;supercomputer;data visualisation;mainframes;parallel machines;},
doi={10.1109/E-SCIENCE.2007.21},
ISSN={},}

@INPROCEEDINGS{vishwanath:2011:glean,
author={Vishwanath, V. and Hereld, M. and Papka, M.E.},
booktitle={Large Data Analysis and Visualization (LDAV), 2011 IEEE Symposium on}, title={Toward simulation-time data analysis and I/O acceleration on leadership-class systems},
year={2011},
month={oct.},
volume={},
number={},
pages={9 -14},
abstract={The performance mismatch between computing and I/O components of current-generation HPC systems has made I/O the critical bottleneck for scientific applications. It is therefore critical to make data movement as efficient as possible, and, to facilitate simulation-time data analysis and visualization to reduce the data written to storage. These will be of paramount importance to enabling us to glean novel insights from simulations. We present our work in GLEAN, a flexible framework for data-analysis and I/O acceleration at extreme scale. GLEAN leverages the data semantics of applications, and fully exploits the diverse system topologies and characteristics. We discuss the performance of GLEAN for simulation-time analysis and I/O acceleration with simulations at scale on leadership class systems.},
keywords={GLEAN;I/O acceleration;current-generation HPC systems;leadership-class systems;simulation-time data analysis;data analysis;input-output programs;},
doi={10.1109/LDAV.2011.6092178},
ISSN={},}

@string{hiperio = {High Performance {I/O} Techniques and Deployment of Very Large Scale {I/O} Systems}}

@string{hiperio2006 = {Proceedings of the 2006 International Workshop on } # hiperio}

@inproceedings{oldfield:lwfs-data-movement,
        Abstract = {Efficient data movement is an important part of any high-performance
        I/O system, but it is especially critical for the current and next-generation
        of massively parallel processing (MPP) systems. In this paper, we
        discuss how the scale, architecture, and organization of current
        and proposed MPP systems impact the design of the data-movement scheme
        for the I/O system. We also describe and analyze the approach used
        by the Lightweight File Systems (LWFS) project, and we compare that
        approach to more conventional data-movement protocols used by small
        and mid-range clusters. Our results indicate that the data-movement
        strategy used by LWFS clearly outperforms conventional data-movement
        protocols, particularly as data sizes increase.},
        Address = {Barcelona, Spain},
        Author = {Ron A. Oldfield and Patrick Widener and Arthur B. Maccabe and Lee Ward and Todd Kordenbrock},
        Booktitle = hiperio2006,
        Date-Modified = {2011-03-31 11:15:20 -0600},
        Doi = {10.1109/CLUSTR.2006.311897},
        Institution = {Sandia National Laboratories},
        Keywords = {lightweight storage, data movement, scalable-io, Portals, LWFS, pario-bib},
        Month = sep,
        Owner = {raoldfi},
        Timestamp = {2006.05.15},
        Title = {Efficient Data-Movement for Lightweight {I/O}},
        Url = {http://doi.ieeecomputersociety.org/10.1109/CLUSTR.2006.311897},
        Vitatype = {refConference},
        Year = {2006},
        Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/CLUSTR.2006.311897},
        Bdsk-Url-2 = {http://dx.doi.org/10.1109/CLUSTR.2006.311897}}

@inproceedings{Soumagne:2013:mercury,
  added-at = {2014-01-13T00:00:00.000+0100},
  author = {Soumagne, Jerome and Kimpe, Dries and Zounmevo, Judicael A. and Chaarawi, Mohamad and Koziol, Quincey and Afsahi, Ahmad and Ross, Robert B.},
  biburl = {http://www.bibsonomy.org/bibtex/28d70ae16d7724e75ad9cd7a801b5d45d/dblp},
  booktitle = {CLUSTER},
  ee = {http://dx.doi.org/10.1109/CLUSTER.2013.6702617},
  abstract={Remote procedure call (RPC) is a technique that has been largely adopted by distributed services. This technique, now more and more used in the context of high-performance computing (HPC), allows the execution of routines to be delegated to remote nodes, which can be set aside and dedicated to specific tasks. However, existing RPC frameworks assume a socket-based network interface (usually on top of TCP/IP), which is not appropriate for HPC systems, because this API does not typically map well to the native network transport used on those systems, resulting in lower network performance. In addition, existing RPC frameworks often do not support handling large data arguments, such as those found in read or write calls. We present in this paper an asynchronous RPC interface, called Mercury, specifically designed for use in HPC systems. The interface allows asynchronous transfer of parameters and execution requests and provides direct support of large data arguments. Mercury is generic in order to allow any function call to be shipped. Additionally, the network implementation is abstracted, allowing easy porting to future systems and efficient use of existing native transport mechanisms.},
  interhash = {fe5e6fdd94f78c26c2713b543b8b73c3},
  intrahash = {8d70ae16d7724e75ad9cd7a801b5d45d},
  keywords = {dblp},
  pages = {1-8},
  publisher = {IEEE},
  timestamp = {2014-01-13T00:00:00.000+0100},
  title = {Mercury: Enabling remote procedure call for high-performance computing.},
  url = {http://dblp.uni-trier.de/db/conf/cluster/cluster2013.html#SoumagneKZCKAR13},
  year = 2013
}

@inproceedings{dayal:2014:flexpath,
 author = {Jai Dayal and Drew Bratcher and Hasan Abbasi and Greg Eisenhauer and Scott Klasky and Norbert Podhorszki and Karsten Schwan and Matthew Wolf},
 title = {{Flexpath: Type-Based Publish/Subscribe System for Large-scale Science Analytics}},
 booktitle = {Cluster, Cloud, and Grid},
 series = {CCGrid '14},
 year = {2014},
 location = {Chicago, Illinois},
 numpages = {10},
 publisher = {IEEE},
 keywords = {I/O, Publish Subscribe, code coupling, data redistribution, workflows}
}

@inproceedings{plimpton:1997:lammps,
  author    = {Steve Plimpton and
               Roy Pollock and
               Mark Stevens},
  title     = {Particle-Mesh Ewald and rRESPA for Parallel Molecular Dynamics Simulations},
  booktitle = {Proceedings of the Eighth {SIAM} Conference on Parallel Processing
               for Scientific Computing, {PPSC} 1997, March 14-17, 1997, Hyatt Regency
               Minneapolis on Nicollel Mall Hotel, Minneapolis, Minnesota, {USA}},
  year      = {1997},
  crossref  = {DBLP:conf/ppsc/1997},
  timestamp = {Fri, 25 Apr 2003 07:51:56 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ppsc/PlimptonPS97},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@proceedings{DBLP:conf/ppsc/1997,
  title     = {Proceedings of the Eighth {SIAM} Conference on Parallel Processing
               for Scientific Computing, {PPSC} 1997, March 14-17, 1997, Hyatt Regency
               Minneapolis on Nicollel Mall Hotel, Minneapolis, Minnesota, {USA}},
  publisher = {{SIAM}},
  year      = {1997},
  isbn      = {0-89871-395-1},
  timestamp = {Fri, 25 Apr 2003 07:51:56 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ppsc/1997},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{lin:gtc,
        Author = {Z. Lin and T. S. Hahm and W. W. Lee and W. M. Tang and R. B. White},
        Date-Added = {2010-03-16 18:06:23 -0600},
        Date-Modified = {2010-03-16 18:08:25 -0600},
        Doi = {10.1126/science.281.5384.1835},
        Journal = {Science},
        Keywords = {GTC, pario-app},
        Month = {September},
        Number = {5384},
        Pages = {1835--1837},
        Title = {Turbulent Transport Reduction by Zonal Flows: Massively Parallel Simulations},
        Volume = {281},
        Year = {1998},
        Bdsk-Url-1 = {http://dx.doi.org/10.1126/science.281.5384.1835}}

@article{hess2008gromacs,
  title={GROMACS 4: algorithms for highly efficient, load-balanced, and scalable molecular simulation},
  author={Hess, Berk and Kutzner, Carsten and Van Der Spoel, David and Lindahl, Erik},
  journal={Journal of chemical theory and computation},
  volume={4},
  number={3},
  pages={435--447},
  year={2008},
  publisher={ACS Publications}
}


@article{racine:2006:gnuplot,
  title={gnuplot 4.0: a portable interactive plotting utility},
  author={Racine, Jeff},
  journal={Journal of Applied Econometrics},
  volume={21},
  number={1},
  pages={133--141},
  year={2006},
  publisher={Wiley Online Library}
}

@INPROCEEDINGS{wolf:2002:smartpointer,
author={M. Wolf and Zhongtang Cai and Weiyun Huang and K. Schwan},
booktitle={Supercomputing, ACM/IEEE 2002 Conference},
title={SmartPointers: Personalized Scientific Data Portals In Your Hand},
year={2002},
pages={20-20},
abstract={The SmartPointer system provides a paradigm for utilizing multiple light-weight client endpoints in a real-time scientific visualization infrastructure. Together, the client and server infrastructure form a new type of data portal for scientific computing. The clients can be used to personalize data for the needs of the individual scientist. This personalization of a shared dataset is designed to allow multiple scientists, each with their laptops or iPaqs to explore the dataset from different angles and with different personalized filters. As an example, iPaq clients can display 2D derived data functions which can be used to dynamically update and annotate the shared data space, which might be visualized separately on a large immersive display such as a CAVE. Measurements are presented for such a system, built upon the ECho middleware system developed at Georgia Tech.},
keywords={Collaboration;Collaborative software;Collaborative work;Computational modeling;Data visualization;Filters;Instruments;Portable computers;Portals;Two dimensional displays},
doi={10.1109/SC.2002.10003},
ISSN={1063-9535},
month={Nov},}

@article{docan:2010:dataspaces,
author = {C. Docan and M. Parashar and S. Klasky},
title = {{DataSpaces}: An Interaction and Coordination Framework for Coupled Simulation Workflows},
abstract = {Emerging high-performance distributed computing environ-
ments are enabling new end-to-end formulations in science
and engineering that involve multiple interacting processes
and data-intensive application workflows. For example, cur-
rent fusion simulation efforts are exploring coupled models
and codes that simultaneously simulate separate application
processes, such as the core and the edge turbulence, and run
on different high performance computing resources. These
components need to interact, at runtime, with each other
and with services for data monitoring, data analysis and vi-
sualization, and data archiving. As a result, they require effi-
cient support for dynamic and flexible couplings and interac-
tions, which remains a challenge. This paper presents Data-
Spaces, a flexible interaction and coordination substrate that
addresses this challenge. DataSpaces essentially implements
a semantically specialized virtual shared space abstraction
that can be associatively accessed by all components and
services in the application workflow. It enables live data
to be extracted from running simulation components, in-
dexes this data online, and then allows it to be monitored,
queried and accessed by other components and services via
the space using semantically meaningful operators. The un-
derlying data transport is asynchronous, low-overhead and
largely memory-to-memory. The design, implementation,
and experimental evaluation of DataSpaces using a coupled
fusion simulation workflow is presented.},
journal = {HPDC '10: Proceedings of the 18th international symposium on High performance distributed computing},
year = {2010},
address = {Chicago, IL, USA},
}

@INPROCEEDINGS{dayal:2015:escience,
author={J. Dayal and J. Lofstead and G. Eisenhauer and K. Schwan and M. Wolf and H. Abbasi and S. Klasky},
booktitle={e-Science (e-Science), 2015 IEEE 11th International Conference on},
title={SODA: Science-Driven Orchestration of Data Analytics},
year={2015},
pages={475-484},
abstract={As scientific simulation applications evolve on the path towards exascale, a new model of scientific inquiry is required where concurrently with the running simulation, online analytics operate on the data it produces. By avoiding offline data storage except when absoluately necessary, it enables speeding up the scientific discovery process by providing rapid insights into the simulated science phenomena and affording more frequent, detailed data analytics than is possible with the traditional purely offline approach of using disk for intermediate data storage. However, a challenge for online analytics is to respond to behavior dynamics caused by changing simulation outputs and by unforeseen events on the underlying hardware/software platforms. This paper presents SODA, a set of run-time abstractions for online orchestration of data analytics, realized by embedding analytics tasks into workstations that monitor component behavior and enable responses to run-time changes in their resource demands and in the platform's resource availability. For high end simulations running on a leadership class machine, experimental evaluations show SODA can invoke efficient orchestration operations responding to a diverse set of run-time dynamics at different granularities to meet end-user and analysis specific requirements.},
keywords={Internet;data analysis;hardware-software codesign;object-oriented programming;scientific information systems;storage management;SODA;behavior dynamics;component behavior;data analytics;hardware/software platform;intermediate data storage;offline data storage;online analytics;online orchestration;run-time abstraction;science-driven orchestration;scientific discovery process;scientific inquiry;scientific simulation application;simulated science phenomena;simulation output;Analytical models;Data analysis;Data models;Monitoring;Pipelines;Runtime;Workstations;Data Analytics;Data Staging;Runtime Management;Scalable I/O;Visualization;in-Situ;resource sharing},
doi={10.1109/eScience.2015.59},
month={Aug},}

@INPROCEEDINGS{lofstead:2012:txn,
author={J. Lofstead and J. Dayal and K. Schwan and R. Oldfield},
booktitle={2012 IEEE International Conference on Cluster Computing},
title={D2T: Doubly Distributed Transactions for High Performance and Distributed Computing},
year={2012},
pages={90-98},
abstract={Current exascale computing projections suggest rather than a monolithic simulation running for the majority of the machine, a collection of components comprising the scientific discovery process will be employed in an online workflow. This move to an online workflow scenario requires knowledge that inter-step operations are completed and correct before the next phase begins. Further, dynamic load balancing or fault tolerance techniques may dynamically deploy or redeploy resources for optimal use of computing resources. These newly configured resources should only be used if they are successfully deployed. Our D2T system offers a mechanism to support these kinds of operations by providing database-like transactions with distributed servers and clients. Ultimately, with adequate hardware support, full ACID compliance is possible for the transactions. To prove the viability of this approach, we show that the D2T protocol has less than 1.2 seconds of overhead using 4096 clients and 32 servers with good scaling characteristics using this initial prototype implementation.},
keywords={distributed processing;fault tolerant computing;natural sciences computing;protocols;resource allocation;ACID compliance;D2T protocol;database-like transactions;distributed computing;distributed servers;doubly distributed transactions;dynamic load balancing;exascale computing projections;fault tolerance techniques;high performance computing;inter-step operations;monolithic simulation;online workflow;scientific discovery process;Computational modeling;Couplings;Fault tolerance;Fault tolerant systems;Protocols;Scalability;Servers;HPC;IO;IO tuning;data staging;exascale;transaction},
doi={10.1109/CLUSTER.2012.79},
ISSN={1552-5244},
month={Sept},}

@inproceedings{lofstead:2013:txn-pdsw,
 author = {Lofstead, Jay and Dayal, Jai and Jimenez, Ivo and Maltzahn, Carlos},
 title = {Efficient Transactions for Parallel Data Movement},
 booktitle = {Proceedings of the 8th Parallel Data Storage Workshop},
 series = {PDSW '13},
 year = {2013},
 isbn = {978-1-4503-2505-9},
 location = {Denver, Colorado},
 pages = {1--6},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2538542.2538567},
 doi = {10.1145/2538542.2538567},
 acmid = {2538567},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{lofstead:2014:txn,
 author = {Lofstead, Jay and Dayal, Jai and Jimenez, Ivo and Maltzahn, Carlos},
 title = {Efficient, Failure Resilient Transactions for Parallel and Distributed Computing},
 booktitle = {Proceedings of the 2014 International Workshop on Data Intensive Scalable Computing Systems},
 series = {DISCS '14},
 year = {2014},
 isbn = {978-1-4799-7038-4},
 location = {New Orleans, Louisiana},
 pages = {17--24},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/DISCS.2014.13},
 doi = {10.1109/DISCS.2014.13},
 acmid = {2689688},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@INPROCEEDINGS{lofstead:2012:txn-metadata,
    author = {Jay Lofstead and Jai Dayal},
    title = {Transactional Parallel Metadata Services for Application Workdflows},
    abstract = {Scientific simulations have a different relationship with all of the data generated than many data analysis systems that support applications like the Large Hadron Collider and the SLOAN Sky Survey. In many cases, simulations need to generate large number of intermediate data sets that ultimately are thrown away once some analysis routines are applied to the data. This generates some summarized, derived result that inspires some scientific insight. Traditionally, these routines use the storage array to persist the intermediate results between each step of the data analysis process. The volume and frequency of this data can be overwhelming compared with the available IO bandwidth on the machine. To handle this volume and frequency, current research efforts are determining how to move the storage of intermediate data from the storage array into the memory of the compute area. Then, the analysis routines are incorporated to create Integrated Application Workflows (IAWs). Data staging techniques require some mechanism to replace the semantics offered by the file system to control data movement and access. As part of an HPC-focused transaction services project, a first pass at a transactional metadata service for in compute area data storage is being developed.},
    booktitle = {In Proceedings of High Performance Computing Meets Databases at Supercomputing},
    year = {2012}
}

@inproceedings{lofstead:2016:superglue,
title={SuperGlue: Standardizing Glue Components for HPC Workflows},
author={Lofstead, Jay and Champsaur, Alexis and Dayal, Jai and Wolf, Matthew and Eisenhauer, Greg},
booktitle={IEEE Cluster 2016},
Address={Taipei, Taiwan},
Month={September},
Year={2016},
}

@techreport{deelman:2015:workflows-report,
   author = "Ewa Deelman and Tom Peterka and Ilkay Altintas and Christopher Carothers and Kerstin Kleese van Dam and Kenneth Moreland and Manish Parashar and Lavanya Ramakrishnan and Michela Taufer and Jeffrey Vetter",
   title = "{The Future of Scientific Workflows Report of the DOE NGNS/CS Scientific Workflows Workshop}",
   month = "April",
   year = "2015",
   location = "Rockville, MD",
   url = "http://science.energy.gov/~/media/ascr/pdf/programdocuments/docs/workflows_final_report.pdf"
}

@article{deelman:2015:pegasus,
author={Ewa Deelman and Karan Vahi and Gideon Juve and Mats Rynge and Scott Callaghan and Philip J Maechling and Rajiv Mayani and Weiwei Chen and Ferreira da Silva, Rafael and Miron Livny and Kent Wenger},
url={http://pegasus.isi.edu/publications/2014/2014-fgcs-deelman.pdf},
title={Pegasus: a Workflow Management System for Science Automation},
journal={Future Generation Computer Systems},
volume={46},
pages={17--35},
year={2015},
note={Funding Acknowledgements: NSF ACI SDCI 0722019, NSF ACI SI2-SSI 1148515 and NSF OCI-1053575},
doi={10.1016/j.future.2014.10.008}
}

@article{tejedor:2015:pycompss,
author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M and Torres, Jordi and Cortes, Toni and Labarta, Jes√∫s},
title = {PyCOMPSs: Parallel computational workflows in Python},
year = {2015},
doi = {10.1177/1094342015594678},
URL = {http://hpc.sagepub.com/content/early/2015/08/19/1094342015594678.abstract},
eprint = {http://hpc.sagepub.com/content/early/2015/08/19/1094342015594678.full.pdf+html},
journal = {International Journal of High Performance Computing Applications}
}

@inproceedings{dorier:2015:in-situ-lessons,
  title={Lessons learned from building in situ coupling frameworks},
  author={Dorier, Matthieu and Dreher, Matthieu and Peterka, Tom and Wozniak, Justin M and Antoniu, Gabriel and Raffin, Bruno},
  booktitle={Proceedings of the First Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},
  pages={19--24},
  year={2015},
  organization={ACM}
}

@article{wilde2011swift,
  title={Swift: A language for distributed parallel scripting},
  author={Wilde, Michael and Hategan, Mihael and Wozniak, Justin M and Clifford, Ben and Katz, Daniel S and Foster, Ian},
  journal={Parallel Computing},
  volume={37},
  number={9},
  pages={633--652},
  year={2011},
  publisher={Elsevier}
}

@misc{champsaur:superglue-repo,
  author = {Champsaur, A.},
  title = {SuperGlue Code},
  year = {2016},
  publisher = {Bitbucket},
  journal = {Bitbucket repository},
  howpublished = {\url{https://acham_@bitbucket.org/acham_/superglue.git}}
}