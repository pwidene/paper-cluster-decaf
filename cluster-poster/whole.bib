@article{bertram:2006:kepler,
 author = {Bertram Lud\"{a}scher and Ilkay Altintas and Chad Berkley and Dan
Higgins and Efrat Jaeger and Matthew Jones and Edward A. Lee and Jing Tao and
Yang Zhao},
 title = {Scientific Workflow Management and the Kepler System:
Research Articles},
 journal = {Concurr. Comput. : Pract. Exper.},
 volume = {18},
 number = {10},
 year = {2006},
 issn = {1532-0626},
 pages = {1039--1065},
 doi = {http://dx.doi.org/10.1002/cpe.v18:10},
 publisher = {John Wiley and Sons Ltd.},
 address = {Chichester, UK, UK}
 }

@ARTICLE{Malewicz:2006:dagman,
title={A Tool for Prioritizing {DAGMan} Jobs and Its Evaluation},
author={Malewicz, G. and Foster, I. and Rosenberg, A.L. and Wilde, M.},
journal={High Performance Distributed Computing, 2006 15th IEEE International Symposium on},
year={2006},
month={0-0 },
volume={},
number={},
pages={156-168},
abstract={It is often difficult to perform efficiently a collection of jobs with complex job dependencies due to temporal unpredictability of the grid. One way to mitigate the unpredictability is to schedule job execution in a manner that constantly maximizes the number of jobs that can be sent to workers. A recently developed scheduling theory provides a basis to meet that optimization goal. Intuitively, when the number of such jobs is always large, high parallelism can be maintained, even if the number of workers changes over time in an unpredictable manner. In this paper we present the design, implementation, and evaluation of a practical scheduling tool inspired by the theory. Given a DAGMan input file with interdependent jobs, the tool prioritizes the jobs. The resulting schedule significantly outperforms currently used schedules under a wide range of system parameters, as shown by simulation studies. For example, a scientific data analysis application, AIRSN, was executed at least 13\% faster with 95\% confidence. An implementation of the tool was integrated with the Condor high-throughput computing system},
keywords={Internet, grid computing, schedulingCondor high-throughput computing system, DAGMan job scheduling, Internet-based computation, job dependency, scheduling tool, scientific data analysis application},
doi={10.1109/HPDC.2006.1652146},
ISSN={1082-8907}, }

@inproceedings{nisar:2008:staging,
 author = {Nisar,, Arifa and Liao,, Wei-keng and Choudhary,, Alok},
 title = {Scaling Parallel {I/O} Performance Through {I/O} Delegate and Caching System},
 booktitle = {SC '08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing},
 year = {2008},
 isbn = {978-1-4244-2835-9},
 pages = {1--12},
 location = {Austin, Texas},
 doi = {http://doi.acm.org.www.library.gatech.edu:2048/10.1145/1413370.1413380},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 }

@inproceedings{abbasi:2009:datastaging,
  author    = {Hasan Abbasi and
               Matthew Wolf and
               Greg Eisenhauer and
               Scott Klasky and
               Karsten Schwan and
               Fang Zheng},
  title     = {DataStager: scalable data staging services for petascale
               applications},
  booktitle = {HPDC},
  year      = {2009},
  pages     = {39-48},
  ee        = {http://doi.acm.org/10.1145/1551609.1551618},
  crossref  = {DBLP:conf/hpdc/2009},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/hpdc/2009,
  editor    = {Dieter Kranzlm{\"u}ller and
               Arndt Bode and
               Heinz-Gerd Hegering and
               Henri Casanova and
               Michael Gerndt},
  title     = {Proceedings of the 18th ACM International Symposium on High
               Performance Distributed Computing, HPDC 2009, Garching,
               Germany, June 11-13, 2009},
  booktitle = {HPDC},
  publisher = {ACM},
  year      = {2009},
  isbn      = {978-1-60558-587-1},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{ober:seismic,
        Abstract = {
A key to reducing the risks and costs associated with oil and gas exploration is the fast, accurate imaging of complex geologies, such as salt domes in the Gulf of Mexico and overthrust regions in U.S. onshore regions. Prestack depth migration generally yields the most accurate images, and one approach to this is to solve the scalar-wave equation using finite differences.
Current industry computational capabilities are insufficient for the application of finite-difference, 3-D, prestack, depth-migration algorithms. A 3-D seismic data can be several terabytes in size, and the multiple runs necessary to refine the velocity model may take many years. The oil companies and seismic contractors need to perform complete velocity field refinements in weeks and single iterations overnight. High-performance computers and state-of-the-art algorithms and software are required to meet this need.
As part of an ongoing ACTI project funded by the U.S. Department of Energy, we have developed a finite-difference, 3-D prestack, depth-migration code for the Intel Paragon. The goal of this work is to demonstrate that massively parallel computers (thousands of processors) can be used efficiently for seismic imaging, and that sufficient computing power exists (or soon will exist) to make finite-difference, prestack, depth migration practical for oil and gas exploration.},
        Author = {Curtis C. Ober and Ron A. Oldfield and David E. Womble and John Van Dyke},
        Date-Added = {2009-10-28 15:13:18 -0600},
        Date-Modified = {2010-11-12 16:11:32 -0700},
        Doi = {DOI: 10.1016/S0898-1221(98)00033-9},
        Issn = {0898-1221},
        Journal = {Computers \& Mathematics with Applications},
        Keywords = {Oil search},
        Note = {Advanced Computing on Intel Architectures},
        Number = {7},
        Pages = {65 - 72},
        Title = {Seismic imaging on the {I}ntel {P}aragon},
        Url = {http://www.sciencedirect.com/science/article/B6TYJ-3SYXG2V-7/2/ce76e3a4ad1f2d8d17e1bdc0751ce513},
        Volume = {35},
        Year = {1998},
        Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/B6TYJ-3SYXG2V-7/2/ce76e3a4ad1f2d8d17e1bdc0751ce513},
        Bdsk-Url-2 = {http://dx.doi.org/10.1016/S0898-1221(98)00033-9}}

@inproceedings{lofstead:2009:adaptable,
  crossref="lofstead:adaptible",
}

@inproceedings{lofstead:adaptible,
        Address = {Rome, Italy},
        Author = {Jay Lofstead and Fang Zheng and Scott Klasky and Karsten Schwan},
        Booktitle = {Proceedings of the International Parallel and Distributed Processing Symposium},
        Date-Added = {2009-10-14 13:52:24 -0600},
        Date-Modified = {2009-10-14 13:55:04 -0600},
        Keywords = {application programmer interface, pario-bib},
        Title = {Adaptable, metadata rich {IO} methods for portable high performance {IO}},
        Year = {2009}}

@INPROCEEDINGS{zheng:2010:predata,
    author = {Fang Zheng and Hasan Abbasi and Ciprian Docan and Jay Lofstead and Scott Klasky and Qing Liu and Manish Parashar and Norbert Podhorszki and Karsten Schwan and Matthew Wolf},
    title = {{PreDatA }- Preparatory Data Analytics on {Peta-Scale} Machines},
    abstract = {Peta-scale scientific applications running on High
End Computing (HEC) platforms can generate large volumes
of data. For high performance storage and in order to be
useful to science end users, such data must be organized in
its layout, indexed, sorted, and otherwise manipulated for subsequent
data presentation, visualization, and detailed analysis.
In addition, scientists desire to gain insights into selected data
characteristics `hidden' or `latent' in these massive datasets
while data is being produced by simulations. PreDatA, short for
Preparatory Data Analytics, is an approach to preparing and
characterizing data while it is being produced by the large scale
simulations running on peta-scale machines. By dedicating
additional compute nodes on the machine as `staging' nodes
and by staging simulations' output data through these nodes,
PreDatA can exploit their computational power to perform
select data manipulations with lower latency than attainable
by first moving data into file systems and storage. Such intransit
manipulations are supported by the PreDatA middleware
through asynchronous data movement to reduce write
latency, application-specific operations on streaming data that
are able to discover latent data characteristics, and appropriate
data reorganization and metadata annotation to speed up
subsequent data access. PreDatA enhances the scalability and
exibility of the current I/O stack on HEC platforms and
is useful for data pre-processing, runtime data analysis and
inspection, as well as for data exchange between concurrently
running simulations.},
    booktitle = {In Proceedings of 24th IEEE International Parallel and Distributed Processing Symposium, April, Atlanta, Georgia},
    year = {2010}
}

@inproceedings{mullender:pegasus,
        Author = {Sape J. Mullender and Ian M. Leslie and Derek McAuley},
        Booktitle = {Proceedings of the 1994 Summer USENIX Technical Conference},
        Comment = {The Pegasus project, the Nemesis operating system, and the Nematode prototype. Nemesis is a single-address-space operating system with multiple protection domains. It is specifically designed for multimedia support, and they have some interesting scheduler support for that. They use a variant of a log-structured file system. They use naming like Plan 9.},
        Keywords = {operating system, multimedia, single address space},
        Pages = {209--219},
        Private = {Skimmed. Not in files; on shelf.},
        Title = {Operating-System Support for Distributed Multimedia},
        Year = {1994}}

@article{Moreland:2008:paraview,
  author={K Moreland and D Lepage and D Koller and G Humphreys},
  title={Remote rendering for ultrascale data},
  journal={Journal of Physics: Conference Series},
  volume={125},
  number={1},
  pages={012096},
  url={http://stacks.iop.org/1742-6596/125/i=1/a=012096},
  year={2008},
  abstract={The mission of the SciDAC Institute for Ultrascale Visualization is to address the upcoming petascale visualization challenges. As we move to petascale computation, we are seeing a trend not only in the growth but also in the consolidation of computing resources. As the distances between user and petascale visualization resources grow, the expected performance of the network degrades, especially with respect to latency. In this paper we will explore a technique for remote visualization that leverages unstructured lumigraph rendering. This technique will provide an interactive rendering experience regardless of the network performance to the remote visualization resource. The unstructured lumigraph rendering can also replace many of the other level-of-detail techniques currently used that have problems that are exasperated by petascale data.}
}

@INPROCEEDINGS{Riedel:2007:visit,
author={Riedel, M. and Eickermann, T. and Habbinga, S. and Frings, W. and Gibbon, P. and Mallmann, D. and Wolf, F. and Streit, A. and Lippert, T. and Schiffmann, W. and Ernst, A. and Spurzem, R. and Nagel, W.E.},
booktitle={e-Science and Grid Computing, IEEE International Conference on}, title={Computational Steering and Online Visualization of Scientific Applications on Large-Scale HPC Systems within e-Science Infrastructures},
year={2007},
month={dec.},
volume={},
number={},
pages={483 -490},
abstract={In the past several years, many scientific applications from various domains have taken advantage of e-science infrastructures that share storage or computational resources such as supercomputers, clusters or PC server farms across multiple organizations. Especially within e-science infrastructures driven by high-performance computing (HPC) such as DEISA, online visualization and computational steering (COVS) has become an important technique to save compute time on shared resources by dynamically steering the parameters of a parallel simulation. This paper argues that future supercomputers in the Petaflop/s performance range with up to 1 million CPUs will create an even stronger demand for seamless computational steering technologies. We discuss upcoming challenges for the development of scalable HPC applications and limits of future storage/IO technologies in the context of next generation e- science infrastructures and outline potential solutions.},
keywords={PC server farm;computational resource;computational steering technology;e-science infrastructure;high performance computing;online visualization;parallel simulation;scientific application;supercomputer;data visualisation;mainframes;parallel machines;},
doi={10.1109/E-SCIENCE.2007.21},
ISSN={},}

@INPROCEEDINGS{vishwanath:2011:glean,
author={Vishwanath, V. and Hereld, M. and Papka, M.E.},
booktitle={Large Data Analysis and Visualization (LDAV), 2011 IEEE Symposium on}, title={Toward simulation-time data analysis and I/O acceleration on leadership-class systems},
year={2011},
month={oct.},
volume={},
number={},
pages={9 -14},
abstract={The performance mismatch between computing and I/O components of current-generation HPC systems has made I/O the critical bottleneck for scientific applications. It is therefore critical to make data movement as efficient as possible, and, to facilitate simulation-time data analysis and visualization to reduce the data written to storage. These will be of paramount importance to enabling us to glean novel insights from simulations. We present our work in GLEAN, a flexible framework for data-analysis and I/O acceleration at extreme scale. GLEAN leverages the data semantics of applications, and fully exploits the diverse system topologies and characteristics. We discuss the performance of GLEAN for simulation-time analysis and I/O acceleration with simulations at scale on leadership class systems.},
keywords={GLEAN;I/O acceleration;current-generation HPC systems;leadership-class systems;simulation-time data analysis;data analysis;input-output programs;},
doi={10.1109/LDAV.2011.6092178},
ISSN={},}

@string{hiperio = {High Performance {I/O} Techniques and Deployment of Very Large Scale {I/O} Systems}}

@string{hiperio2006 = {Proceedings of the 2006 International Workshop on } # hiperio}

@inproceedings{oldfield:lwfs-data-movement,
        Abstract = {Efficient data movement is an important part of any high-performance
        I/O system, but it is especially critical for the current and next-generation
        of massively parallel processing (MPP) systems. In this paper, we
        discuss how the scale, architecture, and organization of current
        and proposed MPP systems impact the design of the data-movement scheme
        for the I/O system. We also describe and analyze the approach used
        by the Lightweight File Systems (LWFS) project, and we compare that
        approach to more conventional data-movement protocols used by small
        and mid-range clusters. Our results indicate that the data-movement
        strategy used by LWFS clearly outperforms conventional data-movement
        protocols, particularly as data sizes increase.},
        Address = {Barcelona, Spain},
        Author = {Ron A. Oldfield and Patrick Widener and Arthur B. Maccabe and Lee Ward and Todd Kordenbrock},
        Booktitle = hiperio2006,
        Date-Modified = {2011-03-31 11:15:20 -0600},
        Doi = {10.1109/CLUSTR.2006.311897},
        Institution = {Sandia National Laboratories},
        Keywords = {lightweight storage, data movement, scalable-io, Portals, LWFS, pario-bib},
        Month = sep,
        Owner = {raoldfi},
        Timestamp = {2006.05.15},
        Title = {Efficient Data-Movement for Lightweight {I/O}},
        Url = {http://doi.ieeecomputersociety.org/10.1109/CLUSTR.2006.311897},
        Vitatype = {refConference},
        Year = {2006},
        Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/CLUSTR.2006.311897},
        Bdsk-Url-2 = {http://dx.doi.org/10.1109/CLUSTR.2006.311897}}

@inproceedings{Soumagne:2013:mercury,
  added-at = {2014-01-13T00:00:00.000+0100},
  author = {Soumagne, Jerome and Kimpe, Dries and Zounmevo, Judicael A. and Chaarawi, Mohamad and Koziol, Quincey and Afsahi, Ahmad and Ross, Robert B.},
  biburl = {http://www.bibsonomy.org/bibtex/28d70ae16d7724e75ad9cd7a801b5d45d/dblp},
  booktitle = {CLUSTER},
  ee = {http://dx.doi.org/10.1109/CLUSTER.2013.6702617},
  abstract={Remote procedure call (RPC) is a technique that has been largely adopted by distributed services. This technique, now more and more used in the context of high-performance computing (HPC), allows the execution of routines to be delegated to remote nodes, which can be set aside and dedicated to specific tasks. However, existing RPC frameworks assume a socket-based network interface (usually on top of TCP/IP), which is not appropriate for HPC systems, because this API does not typically map well to the native network transport used on those systems, resulting in lower network performance. In addition, existing RPC frameworks often do not support handling large data arguments, such as those found in read or write calls. We present in this paper an asynchronous RPC interface, called Mercury, specifically designed for use in HPC systems. The interface allows asynchronous transfer of parameters and execution requests and provides direct support of large data arguments. Mercury is generic in order to allow any function call to be shipped. Additionally, the network implementation is abstracted, allowing easy porting to future systems and efficient use of existing native transport mechanisms.},
  interhash = {fe5e6fdd94f78c26c2713b543b8b73c3},
  intrahash = {8d70ae16d7724e75ad9cd7a801b5d45d},
  keywords = {dblp},
  pages = {1-8},
  publisher = {IEEE},
  timestamp = {2014-01-13T00:00:00.000+0100},
  title = {Mercury: Enabling remote procedure call for high-performance computing.},
  url = {http://dblp.uni-trier.de/db/conf/cluster/cluster2013.html#SoumagneKZCKAR13},
  year = 2013
}

@inproceedings{Dayal:2014:flexpath,
 author = {Jai Dayal and Drew Bratcher and Hasan Abbasi and Greg Eisenhauer and Scott Klasky and Norbert Podhorszki and Karsten Schwan and Matthew Wolf},
 title = {{Flexpath: Type-Based Publish/Subscribe System for Large-scale Science Analytics}},
 booktitle = {Cluster, Cloud, and Grid},
 series = {CCGrid '14},
 year = {2014},
 location = {Chicago, Illinois},
 numpages = {10},
 publisher = {IEEE},
 keywords = {I/O, Publish Subscribe, code coupling, data redistribution, workflows}
}

@inproceedings{plimpton:1997:lammps,
  author    = {Steve Plimpton and
               Roy Pollock and
               Mark Stevens},
  title     = {Particle-Mesh Ewald and rRESPA for Parallel Molecular Dynamics Simulations},
  booktitle = {Proceedings of the Eighth {SIAM} Conference on Parallel Processing
               for Scientific Computing, {PPSC} 1997, March 14-17, 1997, Hyatt Regency
               Minneapolis on Nicollel Mall Hotel, Minneapolis, Minnesota, {USA}},
  year      = {1997},
  crossref  = {DBLP:conf/ppsc/1997},
  timestamp = {Fri, 25 Apr 2003 07:51:56 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ppsc/PlimptonPS97},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@proceedings{DBLP:conf/ppsc/1997,
  title     = {Proceedings of the Eighth {SIAM} Conference on Parallel Processing
               for Scientific Computing, {PPSC} 1997, March 14-17, 1997, Hyatt Regency
               Minneapolis on Nicollel Mall Hotel, Minneapolis, Minnesota, {USA}},
  publisher = {{SIAM}},
  year      = {1997},
  isbn      = {0-89871-395-1},
  timestamp = {Fri, 25 Apr 2003 07:51:56 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ppsc/1997},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{lin:gtc,
        Author = {Z. Lin and T. S. Hahm and W. W. Lee and W. M. Tang and R. B. White},
        Date-Added = {2010-03-16 18:06:23 -0600},
        Date-Modified = {2010-03-16 18:08:25 -0600},
        Doi = {10.1126/science.281.5384.1835},
        Journal = {Science},
        Keywords = {GTC, pario-app},
        Month = {September},
        Number = {5384},
        Pages = {1835--1837},
        Title = {Turbulent Transport Reduction by Zonal Flows: Massively Parallel Simulations},
        Volume = {281},
        Year = {1998},
        Bdsk-Url-1 = {http://dx.doi.org/10.1126/science.281.5384.1835}}


@article{racine:2006:gnuplot,
  title={gnuplot 4.0: a portable interactive plotting utility},
  author={Racine, Jeff},
  journal={Journal of Applied Econometrics},
  volume={21},
  number={1},
  pages={133--141},
  year={2006},
  publisher={Wiley Online Library}
}
